{"cells":[{"cell_type":"code","source":["import os, random, numpy as np, pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, classification_report, precision_recall_curve\n","from joblib import dump\n","from xgboost import XGBClassifier\n","import warnings\n","import joblib\n","warnings.filterwarnings('ignore')\n","\n","# ---- CONFIGURATION ----\n","CONFIG = {\n","    'test_size': 0.20,\n","    'val_size': 0.20,\n","    'random_state': 42,\n","    'early_stopping_rounds': 100,\n","    'n_estimators': 2000,\n","    'learning_rate': 0.03,\n","    'max_depth': 4,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'threshold_precision': 99,  # Number of thresholds to test\n","    'min_samples_leaf': 3,      # Prevent overfitting\n","    'reg_alpha': 0.1,           # L1 regularization\n","    'reg_lambda': 1.0           # L2 regularization\n","}\n","\n","def validate_data(df):\n","    \"\"\"Validate input data structure and quality\"\"\"\n","    assert 'df' in globals() or df is not None, \"DataFrame not found\"\n","    assert 'result' in df.columns, \"Target column 'result' not found\"\n","\n","    # Check for data quality issues\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    print(f\"Dataset shape: {df.shape}\")\n","    print(f\"Numeric features: {len(numeric_cols)-1}\")  # -1 for target\n","    print(f\"Missing values: {df.isnull().sum().sum()}\")\n","    print(f\"Class distribution: {df['result'].value_counts().to_dict()}\")\n","\n","    return df\n","\n","def setup_reproducibility(seed=42):\n","    \"\"\"Set all random seeds for reproducibility\"\"\"\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","def prepare_features_target(df):\n","    \"\"\"Prepare features and target with enhanced preprocessing\"\"\"\n","    y = df['result'].astype(int)\n","\n","    # Select numeric features and handle potential issues\n","    X = df.drop(columns=['result']).select_dtypes(include=[np.number]).copy()\n","\n","    # Handle infinite values\n","    X = X.replace([np.inf, -np.inf], np.nan)\n","\n","    # Fill missing values with median (more robust than mean)\n","    X = X.fillna(X.median())\n","\n","    # Remove constant features (no predictive power)\n","    constant_features = X.columns[X.nunique() <= 1]\n","    if len(constant_features) > 0:\n","        print(f\"Removing {len(constant_features)} constant features\")\n","        X = X.drop(columns=constant_features)\n","\n","    print(f\"Final feature count: {X.shape[1]}\")\n","    return X, y\n","\n","def create_splits(X, y, config):\n","    \"\"\"Create train/validation/test splits\"\"\"\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=config['test_size'],\n","        stratify=y, random_state=config['random_state']\n","    )\n","\n","    X_tr, X_val, y_tr, y_val = train_test_split(\n","        X_train, y_train, test_size=config['val_size'],\n","        stratify=y_train, random_state=config['random_state']\n","    )\n","\n","    return X_tr, X_val, X_test, y_tr, y_val, y_test\n","\n","def calculate_class_weights(y_train):\n","    \"\"\"Calculate class weights for imbalanced data\"\"\"\n","    pos = (y_train == 1).sum()\n","    neg = (y_train == 0).sum()\n","    assert pos > 0 and neg > 0, \"Both classes must be present\"\n","\n","    ratio = neg / pos\n","    print(f\"Class imbalance ratio (neg/pos): {ratio:.2f}\")\n","    return ratio\n","\n","def train_model(X_tr, y_tr, X_val, y_val, scale_pos_weight, config):\n","    \"\"\"Train XGBoost model with optimized parameters\"\"\"\n","    # Base model without early stopping parameters\n","    model = XGBClassifier(\n","        random_state=config['random_state'],\n","        n_jobs=-1,\n","        tree_method=\"hist\",\n","        scale_pos_weight=scale_pos_weight,\n","        eval_metric=\"logloss\",\n","        n_estimators=config['n_estimators'],\n","        learning_rate=config['learning_rate'],\n","        max_depth=config['max_depth'],\n","        subsample=config['subsample'],\n","        colsample_bytree=config['colsample_bytree'],\n","        min_child_weight=config['min_samples_leaf'],\n","        reg_alpha=config['reg_alpha'],\n","        reg_lambda=config['reg_lambda']\n","    )\n","\n","    print(\"Training model with early stopping...\")\n","\n","    # Try different early stopping approaches based on XGBoost version\n","    fit_params = {\n","        'X': X_tr,\n","        'y': y_tr,\n","        'eval_set': [(X_val, y_val)],\n","        'verbose': False\n","    }\n","\n","    try:\n","        # Method 1: Try new callback syntax (XGBoost >= 1.6.0)\n","        from xgboost.callback import EarlyStopping\n","        fit_params['callbacks'] = [EarlyStopping(\n","            rounds=config['early_stopping_rounds'],\n","            save_best=True,\n","            maximize=False\n","        )]\n","        model.fit(**fit_params)\n","        print(\"Used new callback syntax\")\n","\n","    except (TypeError, ImportError, AttributeError):\n","        try:\n","            # Method 2: Try legacy parameter in fit() (XGBoost 1.4-1.5)\n","            fit_params.pop('callbacks', None)  # Remove callbacks if present\n","            fit_params['early_stopping_rounds'] = config['early_stopping_rounds']\n","            model.fit(**fit_params)\n","            print(\"Used legacy fit parameter\")\n","\n","        except TypeError:\n","            # Method 3: Very old XGBoost - train without early stopping\n","            print(\"Early stopping not supported, training with full n_estimators...\")\n","            fit_params = {'X': X_tr, 'y': y_tr}\n","            model.fit(**fit_params)\n","\n","    # Get best iteration (handle different attribute names)\n","    best_iter = getattr(model, 'best_iteration',\n","                       getattr(model, 'best_ntree_limit',\n","                              getattr(model, 'n_estimators', 'N/A')))\n","    print(f\"Training completed. Best iteration: {best_iter}\")\n","    return model\n","\n","def find_optimal_threshold(y_true, y_proba, config, metric='f1'):\n","    \"\"\"Find optimal threshold using specified metric\"\"\"\n","    thresholds = np.linspace(0.01, 0.99, config['threshold_precision'])\n","\n","    if metric == 'f1':\n","        scores = [f1_score(y_true, (y_proba >= t).astype(int)) for t in thresholds]\n","    elif metric == 'precision_recall':\n","        # Use precision-recall curve for better threshold selection\n","        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_proba)\n","        # Find threshold that maximizes F1 score\n","        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n","        best_idx = np.argmax(f1_scores)\n","        return float(pr_thresholds[best_idx] if best_idx < len(pr_thresholds) else 0.5)\n","\n","    best_threshold = float(thresholds[np.argmax(scores)])\n","    best_score = max(scores)\n","\n","    print(f\"Best {metric} score: {best_score:.4f} at threshold: {best_threshold:.3f}\")\n","    return best_threshold\n","\n","def evaluate_model(model, X_val, y_val, X_test, y_test, threshold, config):\n","    \"\"\"Comprehensive model evaluation\"\"\"\n","    # Validation metrics\n","    y_val_proba = model.predict_proba(X_val)[:, 1]\n","    val_ap = average_precision_score(y_val, y_val_proba)\n","    val_roc = roc_auc_score(y_val, y_val_proba)\n","\n","    # Test metrics\n","    y_test_proba = model.predict_proba(X_test)[:, 1]\n","    test_ap = average_precision_score(y_test, y_test_proba)\n","    test_roc = roc_auc_score(y_test, y_test_proba)\n","    y_test_pred = (y_test_proba >= threshold).astype(int)\n","\n","    print(f\"Validation  AP: {val_ap:.4f} | ROC: {val_roc:.4f}\")\n","    print(f\"Test        AP: {test_ap:.4f} | ROC: {test_roc:.4f} | Threshold: {threshold:.3f}\")\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_test_pred, digits=4))\n","\n","    return y_test_proba, {\n","        'val_ap': val_ap, 'val_roc': val_roc,\n","        'test_ap': test_ap, 'test_roc': test_roc,\n","        'threshold': threshold\n","    }\n","\n","def create_feature_importance(model, feature_names, output_dir=\"outputs\"):\n","    \"\"\"Create comprehensive feature importance analysis\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    booster = model.get_booster()\n","\n","    # Multiple importance types for better insight\n","    importance_types = ['gain', 'weight', 'cover']\n","    all_importance = {}\n","\n","    for imp_type in importance_types:\n","        imp_dict = booster.get_score(importance_type=imp_type)\n","        feat_map = {f\"f{i}\": col for i, col in enumerate(feature_names)}\n","        all_importance[imp_type] = {feat_map.get(k, k): v for k, v in imp_dict.items()}\n","\n","    # Create comprehensive importance DataFrame\n","    importance_data = []\n","    for feature in feature_names:\n","        row = {'feature': feature}\n","        for imp_type in importance_types:\n","            row[f'importance_{imp_type}'] = all_importance[imp_type].get(feature, 0)\n","        importance_data.append(row)\n","\n","    fi_df = pd.DataFrame(importance_data).sort_values('importance_gain', ascending=False)\n","\n","    # Add normalized importance (0-100 scale)\n","    for imp_type in importance_types:\n","        col = f'importance_{imp_type}'\n","        fi_df[f'{col}_normalized'] = (fi_df[col] / fi_df[col].max() * 100).round(2)\n","\n","    fi_df.to_csv(f\"{output_dir}/feature_importance_detailed.csv\", index=False)\n","\n","    # Top 20 for Power BI (easier visualization)\n","    fi_top20 = fi_df.head(20)[['feature', 'importance_gain_normalized']].copy()\n","    fi_top20.to_csv(f\"{output_dir}/feature_importance_top20.csv\", index=False)\n","\n","    print(f\"Feature importance saved to {output_dir}/\")\n","    return fi_df\n","\n","def create_powerbi_outputs(model, X_test, y_test, y_test_proba, threshold, metrics, output_dir=\"outputs\"):\n","    \"\"\"Create optimized outputs for Power BI visualization\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # 1. Enhanced predictions with more granular risk categories\n","    risk_bins = [0, threshold*0.4, threshold*0.7, threshold, threshold*1.5, 1.01]\n","    risk_labels = [\"Very Low\", \"Low\", \"Medium\", \"High\", \"Critical\"]\n","    risk_category = pd.cut(y_test_proba, bins=risk_bins, labels=risk_labels, include_lowest=True)\n","\n","    # Create comprehensive predictions dataset\n","    predictions_df = pd.DataFrame({\n","        'row_index': X_test.index,\n","        'actual_result': y_test.values,\n","        'probability_default': np.round(y_test_proba, 4),\n","        'predicted_result': (y_test_proba >= threshold).astype(int),\n","        'risk_category': risk_category,\n","        'risk_score': np.round(y_test_proba * 100, 1),  # 0-100 scale\n","        'confidence_level': pd.cut(y_test_proba,\n","                                 bins=[0, 0.3, 0.7, 1.01],\n","                                 labels=[\"Low\", \"Medium\", \"High\"])\n","    })\n","\n","    # Add decile analysis for better segmentation\n","    predictions_df['risk_decile'] = pd.qcut(y_test_proba, 10, labels=range(1, 11))\n","\n","    predictions_df.to_csv(f\"{output_dir}/predictions_enhanced.csv\", index=False)\n","\n","    # 2. Model performance summary for dashboard KPIs\n","    performance_summary = pd.DataFrame([{\n","        'metric': 'Precision-Recall AUC',\n","        'validation_score': round(metrics['val_ap'], 4),\n","        'test_score': round(metrics['test_ap'], 4),\n","        'description': 'Area under precision-recall curve (primary metric)'\n","    }, {\n","        'metric': 'ROC AUC',\n","        'validation_score': round(metrics['val_roc'], 4),\n","        'test_score': round(metrics['test_roc'], 4),\n","        'description': 'Area under ROC curve'\n","    }, {\n","        'metric': 'Optimal Threshold',\n","        'validation_score': round(threshold, 3),\n","        'test_score': round(threshold, 3),\n","        'description': 'Threshold maximizing F1 score'\n","    }])\n","\n","    performance_summary.to_csv(f\"{output_dir}/model_performance_summary.csv\", index=False)\n","\n","    # 3. Risk distribution analysis\n","    risk_analysis = predictions_df.groupby('risk_category').agg({\n","        'probability_default': ['count', 'mean', 'std'],\n","        'actual_result': ['sum', 'mean']\n","    }).round(4)\n","\n","    risk_analysis.columns = ['_'.join(col).strip() for col in risk_analysis.columns]\n","    risk_analysis = risk_analysis.reset_index()\n","    risk_analysis['default_rate_actual'] = (risk_analysis['actual_result_sum'] /\n","                                          risk_analysis['probability_default_count'] * 100).round(2)\n","\n","    risk_analysis.to_csv(f\"{output_dir}/risk_category_analysis.csv\", index=False)\n","\n","    # 4. Decile analysis for model calibration\n","    decile_analysis = predictions_df.groupby('risk_decile').agg({\n","        'probability_default': ['count', 'min', 'max', 'mean'],\n","        'actual_result': ['sum', 'mean']\n","    }).round(4)\n","\n","    decile_analysis.columns = ['_'.join(col).strip() for col in decile_analysis.columns]\n","    decile_analysis = decile_analysis.reset_index()\n","    decile_analysis['default_rate_actual'] = (decile_analysis['actual_result_sum'] /\n","                                            decile_analysis['probability_default_count'] * 100).round(2)\n","    decile_analysis['default_rate_predicted'] = (decile_analysis['probability_default_mean'] * 100).round(2)\n","\n","    decile_analysis.to_csv(f\"{output_dir}/decile_analysis.csv\", index=False)\n","\n","    print(f\"\\nPower BI outputs created in '{output_dir}/' directory:\")\n","    print(\"- predictions_enhanced.csv (main predictions with risk categories)\")\n","    print(\"- model_performance_summary.csv (KPI metrics)\")\n","    print(\"- risk_category_analysis.csv (risk distribution)\")\n","    print(\"- decile_analysis.csv (model calibration)\")\n","\n","    return predictions_df\n","\n","def save_model_artifacts(model, threshold, feature_names, scale_pos_weight, metrics, output_dir=\"outputs\"):\n","    \"\"\"Save model and metadata for production deployment\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Enhanced model bundle\n","    model_bundle = {\n","        'model': model,\n","        'threshold': threshold,\n","        'features': list(feature_names),\n","        'scale_pos_weight': scale_pos_weight,\n","        'performance_metrics': metrics,\n","        'model_config': CONFIG,\n","        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'feature_count': len(feature_names)\n","    }\n","\n","    dump(model_bundle, f\"{output_dir}/bankruptcy_model_complete.joblib\")\n","\n","    # Create model metadata for documentation\n","    metadata_df = pd.DataFrame([{\n","        'parameter': k,\n","        'value': str(v)\n","    } for k, v in CONFIG.items()])\n","\n","    metadata_df.to_csv(f\"{output_dir}/model_metadata.csv\", index=False)\n","    print(f\"Model artifacts saved to {output_dir}/\")\n","\n","# ---- MAIN EXECUTION ----\n","def main():\n","    # Data validation\n","    df = joblib.load('/content/drive/MyDrive/Bankruptcy Prediction Data/Prepared Dataset/Dataset.pkl')\n","\n","    df = validate_data(df)  # Assumes df exists in global scope\n","\n","    # Setup reproducibility\n","    setup_reproducibility(CONFIG['random_state'])\n","\n","    # Prepare data\n","    X, y = prepare_features_target(df)\n","\n","    # Create splits\n","    X_tr, X_val, X_test, y_tr, y_val, y_test = create_splits(X, y, CONFIG)\n","\n","    # Calculate class weights\n","    scale_pos_weight = calculate_class_weights(y_tr)\n","\n","    # Train model\n","    model = train_model(X_tr, y_tr, X_val, y_val, scale_pos_weight, CONFIG)\n","\n","    # Find optimal threshold (using precision-recall curve for better results)\n","    y_val_proba = model.predict_proba(X_val)[:, 1]\n","    threshold = find_optimal_threshold(y_val, y_val_proba, CONFIG, metric='precision_recall')\n","\n","    # Evaluate model\n","    y_test_proba, metrics = evaluate_model(model, X_val, y_val, X_test, y_test, threshold, CONFIG)\n","\n","    # Create feature importance analysis\n","    fi_df = create_feature_importance(model, X.columns)\n","\n","    # Create Power BI optimized outputs\n","    predictions_df = create_powerbi_outputs(model, X_test, y_test, y_test_proba, threshold, metrics)\n","\n","    # Save complete model artifacts\n","    save_model_artifacts(model, threshold, X.columns, scale_pos_weight, metrics)\n","\n","    return model, threshold, predictions_df, fi_df, metrics\n","\n","# ---- EXECUTION ----\n","if __name__ == \"__main__\" or 'df' in globals():\n","    try:\n","        model, threshold, predictions_df, fi_df, metrics = main()\n","        print(\"\\n✅ Pipeline completed successfully!\")\n","        print(f\"📊 Check the 'outputs/' folder for Power BI ready files\")\n","\n","    except Exception as e:\n","        print(f\"❌ Pipeline failed: {str(e)}\")\n","        raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811},"id":"pRHmffP4SmBt","executionInfo":{"status":"error","timestamp":1754935352629,"user_tz":-120,"elapsed":70663,"user":{"displayName":"Mahdi Torkzaban","userId":"15809237820962324254"}},"outputId":"21794b8a-af40-45b5-f780-cf6dde6477a9"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (255919, 61)\n","Numeric features: 60\n","Missing values: 0\n","Class distribution: {0: 227559, 1: 28360}\n","Final feature count: 60\n","Class imbalance ratio (neg/pos): 8.02\n","Training model with early stopping...\n","Early stopping not supported, training with full n_estimators...\n","Training completed. Best iteration: 2000\n","Validation  AP: 0.8597 | ROC: 0.9721\n","Test        AP: 0.8667 | ROC: 0.9739 | Threshold: 0.799\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9731    0.9737    0.9734     45512\n","           1     0.7877    0.7844    0.7860      5672\n","\n","    accuracy                         0.9527     51184\n","   macro avg     0.8804    0.8790    0.8797     51184\n","weighted avg     0.9526    0.9527    0.9526     51184\n","\n","Feature importance saved to outputs/\n","❌ Pipeline failed: bins must increase monotonically.\n"]},{"output_type":"error","ename":"ValueError","evalue":"bins must increase monotonically.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1795274622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'df'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfi_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✅ Pipeline completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📊 Check the 'outputs/' folder for Power BI ready files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1795274622.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;31m# Create Power BI optimized outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mpredictions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_powerbi_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# Save complete model artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1795274622.py\u001b[0m in \u001b[0;36mcreate_powerbi_outputs\u001b[0;34m(model, X_test, y_test, y_test_proba, threshold, metrics, output_dir)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mrisk_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mrisk_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Very Low\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Low\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"High\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Critical\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mrisk_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrisk_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrisk_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# Create comprehensive predictions dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_increasing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bins must increase monotonically.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     fac, bins = _bins_to_cuts(\n","\u001b[0;31mValueError\u001b[0m: bins must increase monotonically."]}]},{"cell_type":"code","source":["import os, random, numpy as np, pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, classification_report, precision_recall_curve\n","from joblib import dump\n","from xgboost import XGBClassifier\n","import warnings\n","import joblib\n","warnings.filterwarnings('ignore')\n","\n","# ---- CONFIGURATION ----\n","CONFIG = {\n","    'test_size': 0.20,\n","    'val_size': 0.20,\n","    'random_state': 42,\n","    'early_stopping_rounds': 100,\n","    'n_estimators': 2000,\n","    'learning_rate': 0.03,\n","    'max_depth': 4,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'threshold_precision': 99,  # Number of thresholds to test\n","    'min_samples_leaf': 3,      # Prevent overfitting\n","    'reg_alpha': 0.1,           # L1 regularization\n","    'reg_lambda': 1.0           # L2 regularization\n","}\n","\n","def validate_data(df):\n","    \"\"\"Validate input data structure and quality\"\"\"\n","    assert 'df' in globals() or df is not None, \"DataFrame not found\"\n","    assert 'result' in df.columns, \"Target column 'result' not found\"\n","\n","    # Check for data quality issues\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    print(f\"Dataset shape: {df.shape}\")\n","    print(f\"Numeric features: {len(numeric_cols)-1}\")  # -1 for target\n","    print(f\"Missing values: {df.isnull().sum().sum()}\")\n","    print(f\"Class distribution: {df['result'].value_counts().to_dict()}\")\n","\n","    return df\n","\n","def setup_reproducibility(seed=42):\n","    \"\"\"Set all random seeds for reproducibility\"\"\"\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","def prepare_features_target(df):\n","    \"\"\"Prepare features and target with enhanced preprocessing\"\"\"\n","    y = df['result'].astype(int)\n","\n","    # Select numeric features and handle potential issues\n","    X = df.drop(columns=['result']).select_dtypes(include=[np.number]).copy()\n","\n","    # Handle infinite values\n","    X = X.replace([np.inf, -np.inf], np.nan)\n","\n","    # Fill missing values with median (more robust than mean)\n","    X = X.fillna(X.median())\n","\n","    # Remove constant features (no predictive power)\n","    constant_features = X.columns[X.nunique() <= 1]\n","    if len(constant_features) > 0:\n","        print(f\"Removing {len(constant_features)} constant features\")\n","        X = X.drop(columns=constant_features)\n","\n","    print(f\"Final feature count: {X.shape[1]}\")\n","    return X, y\n","\n","def create_splits(X, y, config):\n","    \"\"\"Create train/validation/test splits\"\"\"\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=config['test_size'],\n","        stratify=y, random_state=config['random_state']\n","    )\n","\n","    X_tr, X_val, y_tr, y_val = train_test_split(\n","        X_train, y_train, test_size=config['val_size'],\n","        stratify=y_train, random_state=config['random_state']\n","    )\n","\n","    return X_tr, X_val, X_test, y_tr, y_val, y_test\n","\n","def calculate_class_weights(y_train):\n","    \"\"\"Calculate class weights for imbalanced data\"\"\"\n","    pos = (y_train == 1).sum()\n","    neg = (y_train == 0).sum()\n","    assert pos > 0 and neg > 0, \"Both classes must be present\"\n","\n","    ratio = neg / pos\n","    print(f\"Class imbalance ratio (neg/pos): {ratio:.2f}\")\n","    return ratio\n","\n","def train_model(X_tr, y_tr, X_val, y_val, scale_pos_weight, config):\n","    \"\"\"Train XGBoost model with optimized parameters\"\"\"\n","    # Base model without early stopping parameters\n","    model = XGBClassifier(\n","        random_state=config['random_state'],\n","        n_jobs=-1,\n","        tree_method=\"hist\",\n","        scale_pos_weight=scale_pos_weight,\n","        eval_metric=\"logloss\",\n","        n_estimators=config['n_estimators'],\n","        learning_rate=config['learning_rate'],\n","        max_depth=config['max_depth'],\n","        subsample=config['subsample'],\n","        colsample_bytree=config['colsample_bytree'],\n","        min_child_weight=config['min_samples_leaf'],\n","        reg_alpha=config['reg_alpha'],\n","        reg_lambda=config['reg_lambda']\n","    )\n","\n","    print(\"Training model with early stopping...\")\n","\n","    # Try different early stopping approaches based on XGBoost version\n","    fit_params = {\n","        'X': X_tr,\n","        'y': y_tr,\n","        'eval_set': [(X_val, y_val)],\n","        'verbose': False\n","    }\n","\n","    try:\n","        # Method 1: Try new callback syntax (XGBoost >= 1.6.0)\n","        from xgboost.callback import EarlyStopping\n","        fit_params['callbacks'] = [EarlyStopping(\n","            rounds=config['early_stopping_rounds'],\n","            save_best=True,\n","            maximize=False\n","        )]\n","        model.fit(**fit_params)\n","        print(\"Used new callback syntax\")\n","\n","    except (TypeError, ImportError, AttributeError):\n","        try:\n","            # Method 2: Try legacy parameter in fit() (XGBoost 1.4-1.5)\n","            fit_params.pop('callbacks', None)  # Remove callbacks if present\n","            fit_params['early_stopping_rounds'] = config['early_stopping_rounds']\n","            model.fit(**fit_params)\n","            print(\"Used legacy fit parameter\")\n","\n","        except TypeError:\n","            # Method 3: Very old XGBoost - train without early stopping\n","            print(\"Early stopping not supported, training with full n_estimators...\")\n","            fit_params = {'X': X_tr, 'y': y_tr}\n","            model.fit(**fit_params)\n","\n","    # Get best iteration (handle different attribute names)\n","    best_iter = getattr(model, 'best_iteration',\n","                       getattr(model, 'best_ntree_limit',\n","                              getattr(model, 'n_estimators', 'N/A')))\n","    print(f\"Training completed. Best iteration: {best_iter}\")\n","    return model\n","\n","def find_optimal_threshold(y_true, y_proba, config, metric='f1'):\n","    \"\"\"Find optimal threshold using specified metric\"\"\"\n","    thresholds = np.linspace(0.01, 0.99, config['threshold_precision'])\n","\n","    if metric == 'f1':\n","        scores = [f1_score(y_true, (y_proba >= t).astype(int)) for t in thresholds]\n","    elif metric == 'precision_recall':\n","        # Use precision-recall curve for better threshold selection\n","        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_proba)\n","        # Find threshold that maximizes F1 score\n","        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n","        best_idx = np.argmax(f1_scores)\n","        return float(pr_thresholds[best_idx] if best_idx < len(pr_thresholds) else 0.5)\n","\n","    best_threshold = float(thresholds[np.argmax(scores)])\n","    best_score = max(scores)\n","\n","    print(f\"Best {metric} score: {best_score:.4f} at threshold: {best_threshold:.3f}\")\n","    return best_threshold\n","\n","def evaluate_model(model, X_val, y_val, X_test, y_test, threshold, config):\n","    \"\"\"Comprehensive model evaluation\"\"\"\n","    # Validation metrics\n","    y_val_proba = model.predict_proba(X_val)[:, 1]\n","    val_ap = average_precision_score(y_val, y_val_proba)\n","    val_roc = roc_auc_score(y_val, y_val_proba)\n","\n","    # Test metrics\n","    y_test_proba = model.predict_proba(X_test)[:, 1]\n","    test_ap = average_precision_score(y_test, y_test_proba)\n","    test_roc = roc_auc_score(y_test, y_test_proba)\n","    y_test_pred = (y_test_proba >= threshold).astype(int)\n","\n","    print(f\"Validation  AP: {val_ap:.4f} | ROC: {val_roc:.4f}\")\n","    print(f\"Test        AP: {test_ap:.4f} | ROC: {test_roc:.4f} | Threshold: {threshold:.3f}\")\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_test_pred, digits=4))\n","\n","    return y_test_proba, {\n","        'val_ap': val_ap, 'val_roc': val_roc,\n","        'test_ap': test_ap, 'test_roc': test_roc,\n","        'threshold': threshold\n","    }\n","\n","def create_feature_importance(model, feature_names, output_dir=\"outputs\"):\n","    \"\"\"Create comprehensive feature importance analysis\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    booster = model.get_booster()\n","\n","    # Multiple importance types for better insight\n","    importance_types = ['gain', 'weight', 'cover']\n","    all_importance = {}\n","\n","    for imp_type in importance_types:\n","        imp_dict = booster.get_score(importance_type=imp_type)\n","        feat_map = {f\"f{i}\": col for i, col in enumerate(feature_names)}\n","        all_importance[imp_type] = {feat_map.get(k, k): v for k, v in imp_dict.items()}\n","\n","    # Create comprehensive importance DataFrame\n","    importance_data = []\n","    for feature in feature_names:\n","        row = {'feature': feature}\n","        for imp_type in importance_types:\n","            row[f'importance_{imp_type}'] = all_importance[imp_type].get(feature, 0)\n","        importance_data.append(row)\n","\n","    fi_df = pd.DataFrame(importance_data).sort_values('importance_gain', ascending=False)\n","\n","    # Add normalized importance (0-100 scale)\n","    for imp_type in importance_types:\n","        col = f'importance_{imp_type}'\n","        fi_df[f'{col}_normalized'] = (fi_df[col] / fi_df[col].max() * 100).round(2)\n","\n","    fi_df.to_csv(f\"{output_dir}/feature_importance_detailed.csv\", index=False)\n","\n","    # Top 20 for Power BI (easier visualization)\n","    fi_top20 = fi_df.head(20)[['feature', 'importance_gain_normalized']].copy()\n","    fi_top20.to_csv(f\"{output_dir}/feature_importance_top20.csv\", index=False)\n","\n","    print(f\"Feature importance saved to {output_dir}/\")\n","    return fi_df\n","\n","def create_powerbi_outputs(model, X_test, y_test, y_test_proba, threshold, metrics, output_dir=\"outputs\"):\n","    \"\"\"Create optimized outputs for Power BI visualization\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # 1. Enhanced predictions with more granular risk categories\n","    # Create bins ensuring they are monotonically increasing and within [0,1]\n","    bin1 = min(0.2, threshold * 0.4)\n","    bin2 = min(0.4, threshold * 0.7)\n","    bin3 = threshold\n","    bin4 = min(0.9, max(threshold * 1.2, threshold + 0.1))  # Ensure it's above threshold\n","\n","    # Ensure monotonic increasing and valid bounds\n","    risk_bins = [0, bin1, bin2, bin3, bin4, 1.0]\n","    risk_bins = sorted(list(set(risk_bins)))  # Remove duplicates and sort\n","\n","    # Adjust labels based on actual number of bins\n","    if len(risk_bins) == 6:\n","        risk_labels = [\"Very Low\", \"Low\", \"Medium\", \"High\", \"Critical\"]\n","    elif len(risk_bins) == 5:\n","        risk_labels = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n","    elif len(risk_bins) == 4:\n","        risk_labels = [\"Low\", \"Medium\", \"High\"]\n","    else:\n","        risk_labels = [\"Low\", \"High\"]\n","\n","    print(f\"Risk bins: {[round(b, 3) for b in risk_bins]}\")\n","    risk_category = pd.cut(y_test_proba, bins=risk_bins, labels=risk_labels[:len(risk_bins)-1], include_lowest=True)\n","    # Create comprehensive predictions dataset\n","    predictions_df = pd.DataFrame({\n","        'row_index': X_test.index,\n","        'actual_result': y_test.values,\n","        'probability_default': np.round(y_test_proba, 4),\n","        'predicted_result': (y_test_proba >= threshold).astype(int),\n","        'risk_category': risk_category,\n","        'risk_score': np.round(y_test_proba * 100, 1),  # 0-100 scale\n","        'confidence_level': pd.cut(y_test_proba,\n","                                 bins=[0, 0.3, 0.7, 1.0],\n","                                 labels=[\"Low\", \"Medium\", \"High\"],\n","                                 include_lowest=True)\n","    })\n","\n","    # Add decile analysis for better segmentation\n","    predictions_df['risk_decile'] = pd.qcut(y_test_proba, 10, labels=range(1, 11), duplicates='drop')\n","\n","    predictions_df.to_csv(f\"{output_dir}/predictions_enhanced.csv\", index=False)\n","\n","    # 2. Model performance summary for dashboard KPIs\n","    performance_summary = pd.DataFrame([{\n","        'metric': 'Precision-Recall AUC',\n","        'validation_score': round(metrics['val_ap'], 4),\n","        'test_score': round(metrics['test_ap'], 4),\n","        'description': 'Area under precision-recall curve (primary metric)'\n","    }, {\n","        'metric': 'ROC AUC',\n","        'validation_score': round(metrics['val_roc'], 4),\n","        'test_score': round(metrics['test_roc'], 4),\n","        'description': 'Area under ROC curve'\n","    }, {\n","        'metric': 'Optimal Threshold',\n","        'validation_score': round(threshold, 3),\n","        'test_score': round(threshold, 3),\n","        'description': 'Threshold maximizing F1 score'\n","    }])\n","\n","    performance_summary.to_csv(f\"{output_dir}/model_performance_summary.csv\", index=False)\n","\n","    # 3. Risk distribution analysis\n","    risk_analysis = predictions_df.groupby('risk_category').agg({\n","        'probability_default': ['count', 'mean', 'std'],\n","        'actual_result': ['sum', 'mean']\n","    }).round(4)\n","\n","    risk_analysis.columns = ['_'.join(col).strip() for col in risk_analysis.columns]\n","    risk_analysis = risk_analysis.reset_index()\n","    risk_analysis['default_rate_actual'] = (risk_analysis['actual_result_sum'] /\n","                                          risk_analysis['probability_default_count'] * 100).round(2)\n","\n","    risk_analysis.to_csv(f\"{output_dir}/risk_category_analysis.csv\", index=False)\n","\n","    # 4. Decile analysis for model calibration\n","    decile_analysis = predictions_df.groupby('risk_decile').agg({\n","        'probability_default': ['count', 'min', 'max', 'mean'],\n","        'actual_result': ['sum', 'mean']\n","    }).round(4)\n","\n","    decile_analysis.columns = ['_'.join(col).strip() for col in decile_analysis.columns]\n","    decile_analysis = decile_analysis.reset_index()\n","    decile_analysis['default_rate_actual'] = (decile_analysis['actual_result_sum'] /\n","                                            decile_analysis['probability_default_count'] * 100).round(2)\n","    decile_analysis['default_rate_predicted'] = (decile_analysis['probability_default_mean'] * 100).round(2)\n","\n","    decile_analysis.to_csv(f\"{output_dir}/decile_analysis.csv\", index=False)\n","\n","    print(f\"\\nPower BI outputs created in '{output_dir}/' directory:\")\n","    print(\"- predictions_enhanced.csv (main predictions with risk categories)\")\n","    print(\"- model_performance_summary.csv (KPI metrics)\")\n","    print(\"- risk_category_analysis.csv (risk distribution)\")\n","    print(\"- decile_analysis.csv (model calibration)\")\n","\n","    return predictions_df\n","\n","def save_model_artifacts(model, threshold, feature_names, scale_pos_weight, metrics, output_dir=\"outputs\"):\n","    \"\"\"Save model and metadata for production deployment\"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Enhanced model bundle\n","    model_bundle = {\n","        'model': model,\n","        'threshold': threshold,\n","        'features': list(feature_names),\n","        'scale_pos_weight': scale_pos_weight,\n","        'performance_metrics': metrics,\n","        'model_config': CONFIG,\n","        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'feature_count': len(feature_names)\n","    }\n","\n","    dump(model_bundle, f\"{output_dir}/bankruptcy_model_complete.joblib\")\n","\n","    # Create model metadata for documentation\n","    metadata_df = pd.DataFrame([{\n","        'parameter': k,\n","        'value': str(v)\n","    } for k, v in CONFIG.items()])\n","\n","    metadata_df.to_csv(f\"{output_dir}/model_metadata.csv\", index=False)\n","    print(f\"Model artifacts saved to {output_dir}/\")\n","\n","# ---- MAIN EXECUTION ----\n","def main():\n","    # Data validation\n","    df = joblib.load('/content/drive/MyDrive/Bankruptcy Prediction Data/Prepared Dataset/Dataset.pkl')\n","\n","    df = validate_data(df)  # Assumes df exists in global scope\n","\n","    # Setup reproducibility\n","    setup_reproducibility(CONFIG['random_state'])\n","\n","    # Prepare data\n","    X, y = prepare_features_target(df)\n","\n","    # Create splits\n","    X_tr, X_val, X_test, y_tr, y_val, y_test = create_splits(X, y, CONFIG)\n","\n","    # Calculate class weights\n","    scale_pos_weight = calculate_class_weights(y_tr)\n","\n","    # Train model\n","    model = train_model(X_tr, y_tr, X_val, y_val, scale_pos_weight, CONFIG)\n","\n","    # Find optimal threshold (using precision-recall curve for better results)\n","    y_val_proba = model.predict_proba(X_val)[:, 1]\n","    threshold = find_optimal_threshold(y_val, y_val_proba, CONFIG, metric='precision_recall')\n","\n","    # Evaluate model\n","    y_test_proba, metrics = evaluate_model(model, X_val, y_val, X_test, y_test, threshold, CONFIG)\n","\n","    # Create feature importance analysis\n","    fi_df = create_feature_importance(model, X.columns)\n","\n","    # Create Power BI optimized outputs\n","    predictions_df = create_powerbi_outputs(model, X_test, y_test, y_test_proba, threshold, metrics)\n","\n","    # Save complete model artifacts\n","    save_model_artifacts(model, threshold, X.columns, scale_pos_weight, metrics)\n","\n","    return model, threshold, predictions_df, fi_df, metrics\n","\n","# ---- EXECUTION ----\n","if __name__ == \"__main__\" or 'df' in globals():\n","    try:\n","        model, threshold, predictions_df, fi_df, metrics = main()\n","        print(\"\\n✅ Pipeline completed successfully!\")\n","        print(f\"📊 Check the 'outputs/' folder for Power BI ready files\")\n","\n","    except Exception as e:\n","        print(f\"❌ Pipeline failed: {str(e)}\")\n","        raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmP0ii4-TXzo","executionInfo":{"status":"ok","timestamp":1754935553091,"user_tz":-120,"elapsed":75372,"user":{"displayName":"Mahdi Torkzaban","userId":"15809237820962324254"}},"outputId":"d64319d2-3f9c-4fbf-84ac-1cb513831172"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset shape: (255919, 61)\n","Numeric features: 60\n","Missing values: 0\n","Class distribution: {0: 227559, 1: 28360}\n","Final feature count: 60\n","Class imbalance ratio (neg/pos): 8.02\n","Training model with early stopping...\n","Early stopping not supported, training with full n_estimators...\n","Training completed. Best iteration: 2000\n","Validation  AP: 0.8597 | ROC: 0.9721\n","Test        AP: 0.8667 | ROC: 0.9739 | Threshold: 0.799\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9731    0.9737    0.9734     45512\n","           1     0.7877    0.7844    0.7860      5672\n","\n","    accuracy                         0.9527     51184\n","   macro avg     0.8804    0.8790    0.8797     51184\n","weighted avg     0.9526    0.9527    0.9526     51184\n","\n","Feature importance saved to outputs/\n","Risk bins: [0, 0.2, 0.4, 0.799, 0.9, 1.0]\n","\n","Power BI outputs created in 'outputs/' directory:\n","- predictions_enhanced.csv (main predictions with risk categories)\n","- model_performance_summary.csv (KPI metrics)\n","- risk_category_analysis.csv (risk distribution)\n","- decile_analysis.csv (model calibration)\n","Model artifacts saved to outputs/\n","\n","✅ Pipeline completed successfully!\n","📊 Check the 'outputs/' folder for Power BI ready files\n"]}]}],"metadata":{"colab":{"collapsed_sections":["c4sFdU978vDh","jfDg59VAuOAA","8QgyGg-bxAsF","4bPOBOlAw7pd","j0QwyfKn4L2Y"],"provenance":[],"gpuType":"T4","mount_file_id":"1M63FVX8DKR9y2XrQF8gLsk_1bTes64dn","authorship_tag":"ABX9TyPzUmcW1sMDSdnocl941AyL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}